{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                                                             \n",
        "                                                                 \n",
        "                                                                  \n",
        "                                                                   \n",
        "                                                                    \n",
        "                                                                     \n",
        "                                                                      Assignment No:10\n",
        "\n",
        "Name:Shreya Desai\n",
        "\n",
        "PRN:22510049\n",
        "\n",
        "Batch:T3"
      ],
      "metadata": {
        "id": "s-T2R8Awjwyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "from sklearn.model_selection importso train_test_split\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "id": "Iq7b-iUhTgV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZNiyytUOuOb"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(filepath):\n",
        "\n",
        "\n",
        "    # Load dataset\n",
        "    data = pd.read_csv(filepath)\n",
        "    print(f\" Loaded data! Shape: {data.shape}\")\n",
        "\n",
        "    # Drop 'id' column\n",
        "    data = data.drop(columns=['id'])\n",
        "    print(\" Dropped 'id' column.\")\n",
        "\n",
        "    # Check for missing values\n",
        "    print(\"\\n Checking for missing values:\")\n",
        "    print(data.isnull().sum().to_string())\n",
        "\n",
        "    # Encode target column: Diagnosis (M = 1, B = 0)\n",
        "    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\n",
        "    print(\" Encoded 'diagnosis' column: M -> 1, B -> 0\")\n",
        "\n",
        "    # Separate features and target\n",
        "    X = data.drop(columns=['diagnosis'])\n",
        "    y = data['diagnosis']\n",
        "    print(f\" Features shape: {X.shape}, Target shape: {y.shape}\")\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "def skew_train_test_split(X, y, n_move=120, random_state=42):\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Combine X and y for easier row manipulation\n",
        "    train_df = pd.concat([X_train, y_train], axis=1)\n",
        "    test_df = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "    # Get rows where diagnosis is 1 (Malignant)\n",
        "    m_class_rows = train_df[train_df['diagnosis'] == 1]\n",
        "\n",
        "    # Randomly select rows to move\n",
        "    to_move = m_class_rows.sample(n=n_move, random_state=random_state)\n",
        "\n",
        "    # Remove selected malignant rows from train\n",
        "    train_df = train_df.drop(to_move.index)\n",
        "\n",
        "    # Add selected malignant rows to test\n",
        "    test_df = pd.concat([test_df, to_move])\n",
        "\n",
        "    print(f\" Moved {n_move} 'M' (malignant) cases from train to test.\")\n",
        "    print(f\" Final training size: {train_df.shape[0]}, Final test size: {test_df.shape[0]}\")\n",
        "\n",
        "    # Final split into X and y again\n",
        "    X_train = train_df.drop(columns=['diagnosis'])\n",
        "    y_train = train_df['diagnosis']\n",
        "    X_test = test_df.drop(columns=['diagnosis'])\n",
        "    y_test = test_df['diagnosis']\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "G3Ay-cqUQJcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def train_decision_trees(X_train, y_train, n_trees=10):\n",
        "\n",
        "\n",
        "\n",
        "    trees = []\n",
        "    accuracies = []\n",
        "    feature_importances = []\n",
        "\n",
        "    for i in range(n_trees):\n",
        "\n",
        "\n",
        "        # Sample the training data with replacement\n",
        "        indices = np.random.choice(X_train.index, size=len(X_train), replace=True)\n",
        "        X_sample = X_train.loc[indices]\n",
        "        y_sample = y_train.loc[indices]\n",
        "\n",
        "        # Create and train the decision tree\n",
        "        tree = DecisionTreeClassifier(\n",
        "            max_features='sqrt',\n",
        "            class_weight='balanced',\n",
        "            random_state=i\n",
        "        )\n",
        "        tree.fit(X_sample, y_sample)\n",
        "\n",
        "        # Accuracy on full training set\n",
        "        train_preds = tree.predict(X_train)\n",
        "        acc = accuracy_score(y_train, train_preds)\n",
        "\n",
        "        print(f\" Tree {i+1} trained. Accuracy: {acc:.4f}\")\n",
        "\n",
        "        # Save the model and info\n",
        "        trees.append(tree)\n",
        "        accuracies.append(acc)\n",
        "        feature_importances.append(tree.feature_importances_)\n",
        "\n",
        "    return trees, accuracies, feature_importances\n"
      ],
      "metadata": {
        "id": "s04tG1qhRQLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_top_features_simple(importances, feature_names, top_n=10):\n",
        "\n",
        "\n",
        "    # Convert to numpy array for easy averaging\n",
        "    importances_array = np.array(importances)\n",
        "\n",
        "    # Average over all trees\n",
        "    avg_importance = np.mean(importances_array, axis=0)\n",
        "\n",
        "    # Get top N indices\n",
        "    top_indices = np.argsort(avg_importance)[::-1][:top_n]\n",
        "\n",
        "    # Get corresponding feature names\n",
        "    top_features = [feature_names[i] for i in top_indices]\n",
        "\n",
        "    print(f\"\\n Top important feature:\")\n",
        "    for i, feature in enumerate(top_features):\n",
        "        print(f\"{i+1}. {feature}\")\n",
        "\n",
        "\n",
        "    return top_feature\n"
      ],
      "metadata": {
        "id": "89AZjJbTR5mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def retrain_decision_trees_on_selected_features(X_train_selected, y_train, n_trees=10):\n",
        "\n",
        "    print(f\"\\n Retraining {n_trees} decision trees using only selected features...\")\n",
        "\n",
        "    trees = []\n",
        "    accuracies = []\n",
        "\n",
        "    for i in range(n_trees):\n",
        "\n",
        "\n",
        "        # Bootstrap sampling with replacement\n",
        "        indices = np.random.choice(X_train_selected.index, size=len(X_train_selected), replace=True)\n",
        "        X_sample = X_train_selected.loc[indices]\n",
        "        y_sample = y_train.loc[indices]\n",
        "\n",
        "        # Create and train tree\n",
        "        tree = DecisionTreeClassifier(\n",
        "            max_features='sqrt',\n",
        "            class_weight='balanced',\n",
        "            random_state=100 + i\n",
        "        )\n",
        "        tree.fit(X_sample, y_sample)\n",
        "\n",
        "        # Accuracy on full training set\n",
        "        preds = tree.predict(X_train_selected)\n",
        "        acc = accuracy_score(y_train, preds)\n",
        "\n",
        "        print(f\" Retrained Tree {i+1} Accuracy: {acc:.4f}\")\n",
        "\n",
        "        trees.append(tree)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    return trees, accuracies\n"
      ],
      "metadata": {
        "id": "ElQIgrUrSAk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def train_final_models(retrained_trees, X_train_selected, y_train):\n",
        "\n",
        "\n",
        "    # Get predictions of 10 trees on training data\n",
        "    tree_preds = []\n",
        "\n",
        "    for i, tree in enumerate(retrained_trees):\n",
        "        preds = tree.predict(X_train_selected)\n",
        "        tree_preds.append(preds)\n",
        "\n",
        "\n",
        "    # Stack predictions as extra features (shape: n_samples, 10)\n",
        "    tree_preds_matrix = np.column_stack(tree_preds)\n",
        "\n",
        "    # Combine: top features + tree predictions\n",
        "    import pandas as pd\n",
        "    X_combined = pd.concat([X_train_selected.reset_index(drop=True),\n",
        "                            pd.DataFrame(tree_preds_matrix, columns=[f\"tree_{i+1}\" for i in range(len(retrained_trees))])],\n",
        "                            axis=1)\n",
        "\n",
        "    # Model 1: Logistic Regression\n",
        "    log_model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "    log_model.fit(X_combined, y_train)\n",
        "\n",
        "\n",
        "    # Model 2: Master Decision Tree\n",
        "    master_tree = DecisionTreeClassifier(class_weight='balanced', max_features='sqrt', random_state=200)\n",
        "    master_tree.fit(X_combined, y_train)\n",
        "\n",
        "\n",
        "    return log_model, master_tree\n"
      ],
      "metadata": {
        "id": "rzrlX5PASays"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_final_models(retrained_trees, log_model, master_tree, X_test_selected, y_test):\n",
        "\n",
        "\n",
        "    #  Get predictions from each retrained tree on test set\n",
        "    test_tree_preds = []\n",
        "    for i, tree in enumerate(retrained_trees):\n",
        "        preds = tree.predict(X_test_selected)\n",
        "        test_tree_preds.append(preds)\n",
        "\n",
        "\n",
        "    #  Stack predictions horizontally\n",
        "    test_preds_matrix = np.column_stack(test_tree_preds)\n",
        "\n",
        "    #  Combine selected features  tree predictions\n",
        "    X_test_combined = pd.concat([X_test_selected.reset_index(drop=True),\n",
        "                                 pd.DataFrame(test_preds_matrix, columns=[f\"tree_{i+1}\" for i in range(len(retrained_trees))])],\n",
        "                                 axis=1)\n",
        "\n",
        "    #  Make predictions using logistic regression\n",
        "    y_pred_log = log_model.predict(X_test_combined)\n",
        "    acc_log = accuracy_score(y_test, y_pred_log)\n",
        "    # Make predictions using master decision tree\n",
        "    y_pred_tree = master_tree.predict(X_test_combined)\n",
        "    acc_tree = accuracy_score(y_test, y_pred_tree)\n",
        "\n",
        "\n",
        "    #  Print results\n",
        "    print(\"\\n Final Model Evaluation on Test Data:\")\n",
        "    print(f\" Logistic Regression Accuracy: {acc_log:.4f} \")\n",
        "    print(f\" Master Decision Tree Accuracy: {acc_tree:.4f}\")\n",
        "\n",
        "    return acc_log, recall_log, acc_tree, recall_tree\n"
      ],
      "metadata": {
        "id": "rt-GIEaNSs_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "\n",
        "\n",
        "    # Step 1: Load and preprocess data\n",
        "    X, y = load_and_preprocess_data('/content/Cancer_Data.csv')\n",
        "\n",
        "    # Step 2: Split and skew the data\n",
        "    X_train, X_test, y_train, y_test = skew_train_test_split(X, y)\n",
        "\n",
        "\n",
        "    # Step 3: Train 10 decision trees with sample & feature bagging\n",
        "    trees, importances, accuracies = train_decision_trees(X_train, y_train, n_trees=10)\n",
        "\n",
        "    # Step 4: Combine feature importances and select top features\n",
        "    top_features = get_top_features_simple(importances, X_train.columns, top_n=10)\n",
        "\n",
        "# Now select only those features from X_train and X_test\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "\n",
        "\n",
        "    # Step 5: Retrain trees using only selected features\n",
        "    X_train_selected = X_train[top_features]\n",
        "    X_test_selected = X_test[top_features]\n",
        "    retrained_trees, retrained_accuracies = retrain_decision_trees_on_selected_features(X_train_selected, y_train)\n",
        "\n",
        "    # Step 6: Train final logistic model and master decision tree\n",
        "    log_model, master_tree = train_final_models(retrained_trees, X_train_selected, y_train)\n",
        "\n",
        "    # Step 7: Evaluate on test data\n",
        "    evaluate_final_models(retrained_trees, log_model, master_tree, X_test_selected, y_test)\n",
        "\n",
        "\n",
        "\n",
        "main_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ8xHbYwTWkz",
        "outputId": "e306a36b-55aa-471b-c5db-5edb8ad7c353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Loading and preprocessing dataset...\n",
            " Loaded data! Shape: (569, 33)\n",
            " Dropped 'id' column.\n",
            "\n",
            " Checking for missing values:\n",
            "diagnosis                    0\n",
            "radius_mean                  0\n",
            "texture_mean                 0\n",
            "perimeter_mean               0\n",
            "area_mean                    0\n",
            "smoothness_mean              0\n",
            "compactness_mean             0\n",
            "concavity_mean               0\n",
            "concave points_mean          0\n",
            "symmetry_mean                0\n",
            "fractal_dimension_mean       0\n",
            "radius_se                    0\n",
            "texture_se                   0\n",
            "perimeter_se                 0\n",
            "area_se                      0\n",
            "smoothness_se                0\n",
            "compactness_se               0\n",
            "concavity_se                 0\n",
            "concave points_se            0\n",
            "symmetry_se                  0\n",
            "fractal_dimension_se         0\n",
            "radius_worst                 0\n",
            "texture_worst                0\n",
            "perimeter_worst              0\n",
            "area_worst                   0\n",
            "smoothness_worst             0\n",
            "compactness_worst            0\n",
            "concavity_worst              0\n",
            "concave points_worst         0\n",
            "symmetry_worst               0\n",
            "fractal_dimension_worst      0\n",
            "Unnamed: 32                569\n",
            " Encoded 'diagnosis' column: M -> 1, B -> 0\n",
            " Features shape: (569, 31), Target shape: (569,)\n",
            " Moved 120 'M' (malignant) cases from train to test.\n",
            " Final training size: 335, Final test size: 234\n",
            " Tree 1 trained. Accuracy: 0.9851\n",
            " Tree 2 trained. Accuracy: 0.9791\n",
            " Tree 3 trained. Accuracy: 0.9821\n",
            " Tree 4 trained. Accuracy: 0.9731\n",
            " Tree 5 trained. Accuracy: 0.9761\n",
            " Tree 6 trained. Accuracy: 0.9821\n",
            " Tree 7 trained. Accuracy: 0.9731\n",
            " Tree 8 trained. Accuracy: 0.9881\n",
            " Tree 9 trained. Accuracy: 0.9851\n",
            " Tree 10 trained. Accuracy: 0.9851\n",
            "\n",
            " Top 10 important features:\n",
            "1. radius_mean\n",
            "\n",
            " Retraining 10 decision trees using only selected features...\n",
            " Retrained Tree 1 Accuracy: 0.9582\n",
            " Retrained Tree 2 Accuracy: 0.9642\n",
            " Retrained Tree 3 Accuracy: 0.9552\n",
            " Retrained Tree 4 Accuracy: 0.9463\n",
            " Retrained Tree 5 Accuracy: 0.9433\n",
            " Retrained Tree 6 Accuracy: 0.9522\n",
            " Retrained Tree 7 Accuracy: 0.9642\n",
            " Retrained Tree 8 Accuracy: 0.9403\n",
            " Retrained Tree 9 Accuracy: 0.9552\n",
            " Retrained Tree 10 Accuracy: 0.9433\n",
            "\n",
            " Final Model Evaluation on Test Data:\n",
            " Logistic Regression Accuracy: 0.7479, Recall: 0.6728\n",
            " Master Decision Tree Accuracy: 0.7393, Recall: 0.6543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "1)The 10 decision trees worked well, with accuracies between 97% and 99%.\n",
        "\n",
        "\n",
        "2)After using fewer features, the accuracy of the decision trees dropped a little, from 94% to 96%.\n",
        "\n",
        "\n",
        "3)Logistic regression had an accuracy of 74.8% and a recall of 67.3%, meaning it did well at identifying cancer cases.\n",
        "\n",
        "4)he master decision tree had an accuracy of 73.9% and recall of 65.4%, which is lower than logistic regression.\n",
        "\n",
        "5)The logistic regression model did better than the decision trees and the master decision tree in terms of both accuracy and recall."
      ],
      "metadata": {
        "id": "w5EjnSjDlQzB"
      }
    }
  ]
}